{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Financial Document RAG (Retrieval-Augmented Generation) System\n",
        "\n",
        "## Project Overview\n",
        "This notebook demonstrates a Retrieval-Augmented Generation (RAG) system for analyzing financial documents using:\n",
        "- LlamaParse for PDF extraction\n",
        "- Sentence Transformer for embeddings\n",
        "- Pinecone for vector storage\n",
        "- Google Gemini for question answering"
      ],
      "metadata": {
        "id": "WbRZzkPsof6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Required Libraries\n",
        "In this step, we will install the necessary libraries required for the project. These libraries include Pinecone for vector storage, LangChain for managing language models and embeddings, LlamaParse for parsing PDFs, and others needed for our solution."
      ],
      "metadata": {
        "id": "7ZJgbLRTojIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Libraries\n",
        "!pip install --upgrade --quiet pinecone-client pinecone-text pinecone-notebooks langchain-community langchain-huggingface pdfplumber sentence-transformers google-generativeai langchain_community llama_parse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIIxe2nxoxKJ",
        "outputId": "fd69a3ee-c98b-45bf-9347-3ce0689a4c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Importing Necessary Libraries\n",
        "In this step, we will import the libraries needed for our workflow. These libraries include the necessary tools for managing vector stores, embeddings, text parsing, and more.\n",
        "\n",
        "- **os**: For environment variable management.\n",
        "- **nest_asyncio**: To allow asynchronous operations in Jupyter notebooks.\n",
        "- **google.generativeai**: For utilizing Google's generative AI models.\n",
        "- **langchain.embeddings**: For handling embeddings using Sentence Transformers.\n",
        "- **langchain.text_splitter**: For splitting text data into smaller chunks.\n",
        "- **llama_parse**: For parsing PDF documents.\n",
        "- **pinecone**: For managing the vector store and querying data."
      ],
      "metadata": {
        "id": "yaVrSWHCpXZo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y4KT4A3ocJs"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import os\n",
        "import nest_asyncio\n",
        "import google.generativeai as genai\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from llama_parse import LlamaParse\n",
        "from pinecone import Pinecone, ServerlessSpec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Apply nest_asyncio to Prevent Event Loop Issues\n",
        "In this step, we apply `nest_asyncio` to prevent event loop issues when using asynchronous code within Jupyter notebooks. This is important to ensure that our code runs smoothly without any conflicts with the existing event loop in the notebook environment."
      ],
      "metadata": {
        "id": "jHGdisGApd5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply nest_asyncio to prevent event loop issues\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "FqmayeENp9PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Fetch API Keys from Google Colab User Data\n",
        "In this step, we use the `userdata` module from Google Colab to securely fetch API keys for Pinecone and Google Gemini. This helps keep sensitive information, like API keys, secure and prevents hardcoding them directly in the code."
      ],
      "metadata": {
        "id": "Nu57XXalrX9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Fetch API keys securely from Google Colab user data\n",
        "pinecone_api_key = userdata.get('pinecone_api_key')\n",
        "gemini_api_key = userdata.get('gemini_api_key')\n",
        "llama_key = userdata.get('llama_key')"
      ],
      "metadata": {
        "id": "v4wQ33T5qs5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Pinecone Configuration\n",
        "Here we configure Pinecone by creating a Pinecone index if it doesn't already exist. Pinecone will be used to store the document embeddings and facilitate semantic search.\n",
        "\n",
        "- `index_name`: The name of the Pinecone index.\n",
        "- `dimension`: The dimensionality of the vector embeddings, which is set to 384 for the Sentence Transformer model.\n",
        "- `metric`: The similarity metric used for vector comparison, in this case, 'dotproduct'.\n",
        "- `ServerlessSpec`: Specifies the cloud and region for the Pinecone serverless index."
      ],
      "metadata": {
        "id": "7AAvT6w8qB1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pinecone Configuration\n",
        "index_name = 'hybrid-search-langchain-pinecone'\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "# Create Pinecone Index if it doesn't exist\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,  # dimension of dense vector\n",
        "        metric='dotproduct',\n",
        "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "    )\n",
        "\n",
        "# Initialize Pinecone index\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "ksVyeXPcp_Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Configure LlamaParse for PDF Parsing\n",
        "Here, we configure `LlamaParse`, which will be used to parse PDF documents and extract text data in a structured format. The result type is set to \"markdown\" for cleaner formatting."
      ],
      "metadata": {
        "id": "CXEKZJ9Zrv7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LlamaParse\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = llama_key\n",
        "llama_parser = LlamaParse(result_type=\"markdown\")\n",
        "\n",
        "# Load PDF document\n",
        "documents = llama_parser.load_data(\"/content/Sample Financial Statement.pdf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDb-VZynqGFO",
        "outputId": "0b9c004f-2a46-4859-b5d4-73a7406bed78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: parsing_instruction is deprecated. Use complemental_formatting_instruction or content_guideline_instruction instead.\n",
            "Started parsing the file under job_id 571b3c49-c18f-4492-b889-52c51a0ef0b3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Initialize Embedding Model\n",
        "We initialize the `SentenceTransformer` model, which will be used to create embeddings for document content. The embeddings will represent the semantic meaning of the text, enabling efficient similarity search later."
      ],
      "metadata": {
        "id": "Pp_R6UPCr1tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize embedding model\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "n7mqmZJIrzWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Configure Text Splitter\n",
        "The text splitter is configured to split documents into smaller chunks of text. This ensures that we don't exceed the token limit for model processing and also helps preserve context within chunks.\n",
        "\n",
        "- `chunk_size`: The maximum size of each chunk (500 tokens).\n",
        "- `chunk_overlap`: The amount of overlap between chunks to maintain context between them."
      ],
      "metadata": {
        "id": "mrvIAGktr8if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Splitter configuration\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")"
      ],
      "metadata": {
        "id": "vosL3mKzr5hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Prepare Documents for Embedding\n",
        "In this step, we split the documents into smaller text chunks using the text splitter. We then prepare the documents for embedding by creating a list of dictionaries that contains the text chunks and associated metadata (e.g., source)."
      ],
      "metadata": {
        "id": "PVW3FERasB3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare documents for embedding\n",
        "docs = []\n",
        "for doc in documents:\n",
        "    texts = text_splitter.split_text(doc.text)\n",
        "    for text in texts:\n",
        "        docs.append({\n",
        "            'page_content': text,\n",
        "            'metadata': {'source': getattr(doc, 'source', 'Unknown')}\n",
        "        })"
      ],
      "metadata": {
        "id": "y4P0yaT7r_ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Embed Documents for Pinecone Storage\n",
        "We now create embeddings for each document chunk using the `SentenceTransformer` model. These embeddings represent the semantic content of each chunk, allowing us to store them in Pinecone for later retrieval."
      ],
      "metadata": {
        "id": "i4yNYfA7sP7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_documents(docs):\n",
        "    \"\"\"Embed document chunks for Pinecone storage\"\"\"\n",
        "    embedded_docs = []\n",
        "    for doc in docs:\n",
        "        embedding = embeddings.embed_query(doc['page_content'])\n",
        "        embedded_docs.append({\n",
        "            'id': f\"doc_{hash(doc['page_content'])}\",\n",
        "            'values': embedding,\n",
        "            'metadata': {\n",
        "                'text': doc['page_content'],\n",
        "                'source': doc['metadata'].get('source', 'Unknown')\n",
        "            }\n",
        "        })\n",
        "    return embedded_docs"
      ],
      "metadata": {
        "id": "o5iBCICjsNxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Store Embedded Documents to Pinecone\n",
        "This function takes the embedded documents and saves them in batches to the Pinecone index. The `upsert` method is used to insert the embeddings into the vector store."
      ],
      "metadata": {
        "id": "L4hWqT1lsWNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_to_pinecone(embedded_docs):\n",
        "    \"\"\"Save embedded documents to Pinecone in batches\"\"\"\n",
        "    try:\n",
        "        batch_size = 100\n",
        "        for i in range(0, len(embedded_docs), batch_size):\n",
        "            batch = embedded_docs[i:i+batch_size]\n",
        "            index.upsert(vectors=batch)\n",
        "        print(f\"Successfully uploaded {len(embedded_docs)} document chunks\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading to Pinecone: {e}\")\n",
        "\n",
        "# Embed and save documents\n",
        "embedded_docs = embed_documents(docs)\n",
        "store_to_pinecone(embedded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoPxLz0KsUDD",
        "outputId": "faffbb6c-1512-4e63-a0e2-7ff48961b5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully uploaded 491 document chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Retrieve Relevant Information from Pinecone\n",
        "This function queries Pinecone to retrieve relevant documents based on a given query. It uses the query's embedding and returns the top-k most relevant results."
      ],
      "metadata": {
        "id": "r8Cw-uwytINJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_info(query, top_k=5):\n",
        "    \"\"\"Retrieve relevant documents from Pinecone\"\"\"\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    try:\n",
        "        results = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "        return results['matches']\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving documents: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "RUiiLP6_se2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Configure Google Gemini for Generating Responses\n",
        "We configure Google Gemini, which will be used to generate detailed financial analysis responses based on the retrieved documents. We set up various parameters for the generation, including temperature and token limits."
      ],
      "metadata": {
        "id": "o4d_I9VitOXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Google Gemini\n",
        "genai.configure(api_key=gemini_api_key)\n",
        "\n",
        "def generate_response(query):\n",
        "    \"\"\"Generate detailed financial analysis response\"\"\"\n",
        "    relevant_docs = retrieve_relevant_info(query)\n",
        "    context = \"\\n\".join([doc[\"metadata\"][\"text\"] for doc in relevant_docs])\n",
        "\n",
        "    prompt = f\"\"\"You are a financial analyst specializing in profit and loss statements. Based on the financial data provided, answer the following question in a **detailed, sentence-based format**:\n",
        "\n",
        "    **Context:**\n",
        "    {context}\n",
        "\n",
        "    **Query:**\n",
        "    {query}\n",
        "\n",
        "    **Instructions:**\n",
        "    - Provide a clear, well-structured answer.\n",
        "    - If the answer is numerical, explain the context behind the numbers (e.g., percentage increase, variance).\n",
        "    - Keep the response concise but informative, focusing on key metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-pro\")\n",
        "    generation_config = {\n",
        "        \"temperature\": 0.0,\n",
        "        \"top_p\": 0.8,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 1024,\n",
        "        \"candidate_count\": 1\n",
        "    }\n",
        "\n",
        "    safety_settings = [\n",
        "        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n",
        "    ]\n",
        "\n",
        "    response = model.generate_content(\n",
        "        prompt,\n",
        "        generation_config=generation_config,\n",
        "        safety_settings=safety_settings\n",
        "    )\n",
        "\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "BWV9WJrutLzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 14: Example Queries and Responses\n",
        "Finally, we will run some example queries to demonstrate how the system generates responses based on the financial data in the documents."
      ],
      "metadata": {
        "id": "O5dGO96ZtV7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with an example query\n",
        "query = \"What is the gross profit for Q3 2024?\"\n",
        "response = generate_response(query)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "yvKmiiwbtT1S",
        "outputId": "84a0fb13-519b-4c01-a0ed-81eba1b89ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: The gross profit for Q3 2024 is **$46,257**. This represents a **$1,843** increase from the previous quarter and a **$11,843** increase from the same quarter last year. The gross profit margin for Q3 2024 is **67.8%**, which is a slight decrease from the previous quarter but an improvement from the same quarter last year.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with another example query\n",
        "query = \"How do the net income and operating expenses compare for Q1 2024?\"\n",
        "response = generate_response(query)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "BMPV9dmdtbHT",
        "outputId": "e4a013eb-d28b-4ba6-e4c2-8d76d58cdfb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: In Q1 2024, the company's net income experienced a moderate increase of approximately 8.8%, rising from $24,108 million in Q1 2023 to $26,248 million. This represents an absolute increase of $2,140 million.\n",
            "\n",
            "On the other hand, the company's total operating expenses remained relatively stable, with a marginal increase of 0.03% from $14,510 million in Q1 2023 to $14,510 million in Q1 2024. This translates to an absolute increase of only $1 million.\n",
            "\n",
            "Overall, the company's financial performance in Q1 2024 was marked by a modest increase in net income and stable operating expenses, indicating a slight improvement in profitability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XNEs17NBtmEQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}